# ENEL645 Final Project - live American Sign Language detection system

1. The report is [report.pdf](./report.pdf).
1. The main notebook is [asl-model.ipynb](./asl-model.ipynb).
1. The fully trained model is [asl-model.h5](./asl-model.h5).
1. The testing of the model is [asl-model-testing.ipynb](./asl-model-testing.ipynb)
1. The instructions to run it on TALC is [asl-model.slurm](./asl-model.slurm).
1. The video is [video.mp4](https://uofc-my.sharepoint.com/:v:/g/personal/kevin_amadorueda_ucalgary_ca/EXwC852PDMhIlHj-r_jyTAgBixpI7hL9hFN67tuJl2tOMg?e=4h1oeL) (only accessible with a UofC email).
1. The script that opens the camera and detects signs is [live-asl-detection.py](./live-asl-detection.py)

   This is how our detection system looks like:

   ![](./imgs/prediction-c.png)

1. Enjoy!
